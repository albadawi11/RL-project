{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticActor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, action_bound):\n",
    "        super(StochasticActor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x).clamp(-20, 2)  # Clamping for numerical stability\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        x_t = normal.rsample()  # Reparameterization trick\n",
    "        action = torch.tanh(x_t) * self.action_bound\n",
    "        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        return action, log_prob.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q_value = nn.Linear(hidden_dim, 1)  # Output single Q-value\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)  # Concatenate along the feature axis\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.q_value(x)  # Shape [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, state_dim, action_dim, device=\"cpu\"):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = torch.device(device)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.states = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((buffer_size, 1), dtype=np.float32)  # Store rewards with shape [batch_size, 1]\n",
    "        self.next_states = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((buffer_size, 1), dtype=np.float32)  # Store dones with shape [batch_size, 1]\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
    "        self.size = min(self.size + 1, self.buffer_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        indices = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "        torch.tensor(self.states[indices], device=self.device),\n",
    "        torch.tensor(self.actions[indices], device=self.device),\n",
    "        torch.tensor(self.rewards[indices].reshape(-1, 1), device=self.device),  # Shape [batch_size, 1]\n",
    "        torch.tensor(self.next_states[indices], device=self.device),\n",
    "        torch.tensor(self.dones[indices].reshape(-1, 1), device=self.device),    # Shape [batch_size, 1]\n",
    "    )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim, \n",
    "        action_dim, \n",
    "        action_bound, \n",
    "        actor, \n",
    "        critic_1, \n",
    "        critic_2, \n",
    "        buffer, \n",
    "        device, \n",
    "        gamma=0.99, \n",
    "        tau=0.005, \n",
    "        lr=3e-4, \n",
    "        alpha=0.2, \n",
    "        automatic_entropy_tuning=True, \n",
    "        batch_size=256, \n",
    "        updates_per_step=1\n",
    "    ):\n",
    "        self.actor = actor.to(device)\n",
    "        self.critic_1 = critic_1.to(device)\n",
    "        self.critic_2 = critic_2.to(device)\n",
    "        self.target_critic_1 = critic_1.to(device)\n",
    "        self.target_critic_2 = critic_2.to(device)\n",
    "\n",
    "        self.replay_buffer = buffer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.updates_per_step = updates_per_step  # Added updates_per_step attribute\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            self.target_entropy = -action_dim  # Target entropy is heuristic\n",
    "            self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "\n",
    "        # Sync target critics\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "    def select_action(self, state, deterministic=False):\n",
    "        \"\"\"Select an action given the current state.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state.\n",
    "            deterministic (bool): If True, select the mean action deterministically.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The selected action scaled to the environment's action range.\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.actor(state)  # Get the mean and standard deviation\n",
    "            if deterministic:\n",
    "                action = mean\n",
    "            else:\n",
    "                normal = torch.distributions.Normal(mean, std)\n",
    "                action = normal.sample()  # Sample stochastically\n",
    "            action = torch.tanh(action) * self.actor.action_bound  # Scale to the action range\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "\n",
    "    def update(self, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample_batch(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_states)\n",
    "\n",
    "            # Compute Q-values for the next state-action pairs\n",
    "            q1_next = self.target_critic_1(next_states, next_actions)\n",
    "            q2_next = self.target_critic_2(next_states, next_actions)\n",
    "\n",
    "        #print(\"Before torch.min:\")\n",
    "        #print(\"q1_next shape:\", q1_next.shape)  # Should be [batch_size, 1]\n",
    "        #print(\"q2_next shape:\", q2_next.shape)  # Should be [batch_size, 1]\n",
    "\n",
    "        # Compute element-wise minimum\n",
    "        q1_next = q1_next.view(-1, 1)  # Ensure shape [batch_size, 1]\n",
    "        q2_next = q2_next.view(-1, 1)  # Ensure shape [batch_size, 1]\n",
    "        min_q_next = torch.min(q1_next, q2_next)  # Element-wise minimum\n",
    "\n",
    "        #print(\"After torch.min:\")\n",
    "        #print(\"min_q_next shape:\", min_q_next.shape)  # Should be [batch_size, 1]\n",
    "\n",
    "        # Compute target Q-values\n",
    "        rewards = rewards.view(-1, 1)  # Ensure [batch_size, 1]\n",
    "        dones = dones.view(-1, 1)      # Ensure [batch_size, 1]\n",
    "        target_q = rewards + self.gamma * (1 - dones) * min_q_next\n",
    "\n",
    "\n",
    "        # Update Q-functions\n",
    "        current_q1 = self.critic_1(states, actions)\n",
    "        current_q2 = self.critic_2(states, actions)\n",
    "\n",
    "        critic_1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic_2_loss = F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # Update policy\n",
    "        sampled_actions, log_probs = self.actor.sample(states)\n",
    "        q1 = self.critic_1(states, sampled_actions)\n",
    "        q2 = self.critic_2(states, sampled_actions)\n",
    "        actor_loss = (self.alpha * log_probs - torch.min(q1, q2)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update alpha if using automatic entropy tuning\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        # Soft update target networks\n",
    "        self.soft_update(self.target_critic_1, self.critic_1)\n",
    "        self.soft_update(self.target_critic_2, self.critic_2)\n",
    "\n",
    "        return critic_1_loss.item(), critic_2_loss.item(), actor_loss.item()\n",
    "\n",
    "    def soft_update(self, target, source):\n",
    "        \"\"\"Perform a soft update for the target networks.\"\"\"\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * source_param.data + (1 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac_agent(env, agent, episodes, max_steps):\n",
    "    \"\"\"\n",
    "    Training loop for Soft Actor-Critic (SAC) agent.\n",
    "    \n",
    "    Args:\n",
    "        env: Gym environment.\n",
    "        agent: Instance of SACAgent class.\n",
    "        episodes: Number of episodes to train the agent.\n",
    "        max_steps: Maximum number of steps per episode.\n",
    "    \n",
    "    Returns:\n",
    "        rewards_log: List of cumulative rewards per episode.\n",
    "        critic_1_losses, critic_2_losses, actor_losses, timesteps: Lists of losses and timesteps.\n",
    "    \"\"\"\n",
    "    rewards_log = []\n",
    "    critic_1_losses = []\n",
    "    critic_2_losses = []\n",
    "    actor_losses = []\n",
    "    timesteps = []\n",
    "    total_steps = 0  # Count global timesteps\n",
    "\n",
    "    for episode in tqdm(range(1, episodes + 1), desc=\"Training Progress\", unit=\"episode\"):\n",
    "        # Reset environment and retrieve initial state\n",
    "        raw_state, _ = env.reset()\n",
    "        if isinstance(raw_state, dict):\n",
    "            state = raw_state.get(\"observation\", raw_state)\n",
    "        else:\n",
    "            state = raw_state\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "        \n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Select action from policy (with exploration)\n",
    "            action = agent.select_action(state, deterministic=False)\n",
    "\n",
    "            # Step environment\n",
    "            raw_next_state, reward, done, _, info = env.step(action)\n",
    "            if isinstance(raw_next_state, dict):\n",
    "                next_state = raw_next_state.get(\"observation\", raw_next_state)\n",
    "            else:\n",
    "                next_state = raw_next_state\n",
    "            if not isinstance(next_state, np.ndarray):\n",
    "                next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            # Perform updates (if replay buffer has enough samples)\n",
    "            if len(agent.replay_buffer) >= agent.batch_size:\n",
    "                for _ in range(agent.updates_per_step):  # Multiple updates per environment step\n",
    "                    critic_loss_1, critic_loss_2, actor_loss = agent.update()\n",
    "                    if critic_loss_1 is not None and critic_loss_2 is not None and actor_loss is not None:\n",
    "                        critic_1_losses.append(critic_loss_1)\n",
    "                        critic_2_losses.append(critic_loss_2)\n",
    "                        actor_losses.append(actor_loss)\n",
    "                        timesteps.append(total_steps)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards_log.append(episode_reward)\n",
    "        print(f\"Episode {episode}/{episodes}, Reward: {episode_reward}\")\n",
    "\n",
    "    return rewards_log, critic_1_losses, critic_2_losses, actor_losses, timesteps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 1/100 [00:16<26:31, 16.07s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 2/100 [00:42<36:34, 22.39s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/100, Reward: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [01:09<39:25, 24.38s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3/100, Reward: -1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 3/100 [01:36<52:05, 32.22s/episode]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 59\u001b[0m\n\u001b[0;32m     40\u001b[0m     sac_agent \u001b[38;5;241m=\u001b[39m SACAgent(\n\u001b[0;32m     41\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39mstate_dim,\n\u001b[0;32m     42\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39maction_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     updates_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Perform one update per environment step\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Train the SAC Agent\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m train_rewards, critic_1_losses, critic_2_losses, actor_losses, timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sac_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msac_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m     61\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Plot the losses\u001b[39;00m\n\u001b[0;32m     64\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[43], line 53\u001b[0m, in \u001b[0;36mtrain_sac_agent\u001b[1;34m(env, agent, episodes, max_steps)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(agent\u001b[38;5;241m.\u001b[39mupdates_per_step):  \u001b[38;5;66;03m# Multiple updates per environment step\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m         critic_loss_1, critic_loss_2, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m critic_loss_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m critic_loss_2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m actor_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             critic_1_losses\u001b[38;5;241m.\u001b[39mappend(critic_loss_1)\n",
      "Cell \u001b[1;32mIn[42], line 110\u001b[0m, in \u001b[0;36mSACAgent.update\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_1_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    109\u001b[0m critic_1_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_1_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_2_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    113\u001b[0m critic_2_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set seeds\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # Environment\n",
    "    env = gym.make(\"PandaReach-v3\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Dict):\n",
    "        state_dim = env.observation_space[\"observation\"].shape[0]\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high[0]\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = ReplayBuffer(buffer_size=100_000,state_dim=state_dim,action_dim=action_dim)\n",
    "\n",
    "    # Instantiate the actor and critics\n",
    "    actor = StochasticActor(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=256,  # Example hidden layer size\n",
    "    action_bound=action_bound,\n",
    "    )   \n",
    "\n",
    "    critic_1 = Critic(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=256,  # Example hidden layer size\n",
    "    )\n",
    "\n",
    "    critic_2 = Critic(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=256,  # Example hidden layer size\n",
    "    )\n",
    "\n",
    "    # SAC Agent\n",
    "    sac_agent = SACAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_bound=action_bound,\n",
    "    actor=actor,\n",
    "    critic_1=critic_1,\n",
    "    critic_2=critic_2,\n",
    "    buffer=replay_buffer,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    lr=5e-4,\n",
    "    alpha=0.2,\n",
    "    automatic_entropy_tuning=True,\n",
    "    batch_size=256,\n",
    "    updates_per_step=1,  # Perform one update per environment step\n",
    ")\n",
    "\n",
    "# Train the SAC Agent\n",
    "train_rewards, critic_1_losses, critic_2_losses, actor_losses, timesteps = train_sac_agent(\n",
    "    env, sac_agent, episodes=100, max_steps=1000\n",
    ")\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure()\n",
    "plt.plot(timesteps, critic_1_losses, label=\"Critic 1 Loss\")\n",
    "plt.plot(timesteps, critic_2_losses, label=\"Critic 2 Loss\")\n",
    "plt.plot(timesteps, actor_losses, label=\"Actor Loss\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Losses vs Timestep\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the rewards and losses\n",
    "np.save(\"sac_train_rewards.npy\", train_rewards)\n",
    "np.save(\"sac_critic_1_losses.npy\", critic_1_losses)\n",
    "np.save(\"sac_critic_2_losses.npy\", critic_2_losses)\n",
    "np.save(\"sac_actor_losses.npy\", actor_losses)\n",
    "#np.save(\"sac_timesteps.npy\", timesteps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
