{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depandancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, first_dim, second_dim, n_actions,action_low,action_high,alpha, device = \"cpu\"):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.first_dim = first_dim\n",
    "        self.second_dim = second_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.device = device \n",
    "\n",
    "        # Action scaling bounds\n",
    "        self.action_low = torch.tensor(action_low, dtype=torch.float32).to(device)\n",
    "        self.action_high = torch.tensor(action_high, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Define layers\n",
    "        # First layer processes the state input\n",
    "        self.first_layer = nn.Linear(self.input_dim, self.first_dim)\n",
    "\n",
    "        # Second layer processes the intermediate representation\n",
    "        self.second_layer = nn.Linear(self.first_dim, self.second_dim)\n",
    "\n",
    "        # Output layer produces actions\n",
    "        self.output_layer = nn.Linear(self.second_dim, self.n_actions)\n",
    "\n",
    "        # Initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization of weights to zero for first and second layers\n",
    "        nn.init.xavier_uniform_(self.first_layer.weight)\n",
    "        nn.init.zeros_(self.first_layer.bias)\n",
    "        nn.init.xavier_uniform_(self.second_layer.weight)\n",
    "        nn.init.zeros_(self.second_layer.bias)\n",
    "\n",
    "        # Output layer initialized to small values initially\n",
    "        nn.init.uniform_(self.output_layer.weight, -0.003, 0.003)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass of the Actor network.\n",
    "        Args:\n",
    "            state: Tensor of state inputs.\n",
    "        Returns:\n",
    "            Actions bounded between -1 and 1.\n",
    "        \"\"\"\n",
    "        # Pass state through the first layer and apply ReLU activation\n",
    "        first_out = F.relu(self.first_layer(state))\n",
    "\n",
    "        # Pass through the second layer and apply ReLU activation\n",
    "        second_out = F.relu(self.second_layer(first_out))\n",
    "\n",
    "        # Output actions with tanh activation to bound between -1 and 1\n",
    "        action = torch.tanh(self.output_layer(second_out))\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Scale actions to ensure compatability with Panda-Gym's environments\n",
    "\n",
    "    def scale_action(self, action):\n",
    "        \"\"\"\n",
    "        Scale actions from [-1, 1] to [action_low, action_high].\n",
    "        \"\"\"\n",
    "        return self.action_low + (action + 1.0) * 0.5 * (self.action_high - self.action_low)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, first_dim, second_dim, n_actions,alpha, device = \"cpu\"):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Define Dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.first_dim = first_dim\n",
    "        self.second_dim = second_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.device = device  # Store device\n",
    "\n",
    "        # Define layers \n",
    "        self.first_layer = nn.Linear(self.input_dim,self.first_dim)\n",
    "        self.second_layer = nn.Linear(self.first_dim + self.n_actions, self.second_dim)\n",
    "        self.output_layer = nn.Linear(self.second_dim,1)\n",
    "\n",
    "        # Initialization weights\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialize weights in every layer\n",
    "        for layer in [self.first_layer, self.second_layer, self.output_layer]:\n",
    "            # use xavier_unifrom_ for stable learning\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            # intialize to zeros\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self,state,action):\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass of the Critic Network\n",
    "\n",
    "        Args: \n",
    "        State: Tensor of state inputs\n",
    "        Action: Tensor of actions (continous values)\n",
    "\n",
    "        Returns:    \n",
    "            Q-value as a scaler Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Pass the state through the first layer and apply ReLU activation\n",
    "        state_out = F.relu(self.first_layer(state))\n",
    "\n",
    "        # Concatenate the state features with the action input\n",
    "        state_action = torch.cat([state_out, action], dim=1)\n",
    "\n",
    "        # Pass through the second layer and apply ReLU activation\n",
    "        second_out = F.relu(self.second_layer(state_action))\n",
    "\n",
    "        # Output the Q-value\n",
    "        q_value = self.output_layer(second_out)\n",
    "\n",
    "        return q_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    \"\"\"Replay Bufferclass contains:\n",
    "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
    "            - add: a function to add new transition tuple into the buffer\n",
    "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Args:\n",
    "               buffer_size (int): size of the replay buffer\n",
    "        \"\"\"\n",
    "        # total size of the replay buffer\n",
    "        self.total_size = buffer_size\n",
    "\n",
    "        # create a list to store the transitions\n",
    "        self._data_buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_buffer)\n",
    "\n",
    "    def add(self, obs, act, reward, next_obs, done):\n",
    "        # create a tuple\n",
    "        trans = (obs, act, reward, next_obs, done)\n",
    "\n",
    "        if self._next_idx >= len(self._data_buffer):\n",
    "            self._data_buffer.append(trans)\n",
    "        else:\n",
    "            self._data_buffer[self._next_idx] = trans\n",
    "\n",
    "        # increase the index\n",
    "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
    "\n",
    "    def _encode_sample(self, indices, as_tensor = False):\n",
    "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
    "        \n",
    "            Args:\n",
    "                indices (list): list contains the index of all sampled transition tuples.\n",
    "        \"\"\"\n",
    "\n",
    "        obs_shape = self._data_buffer[0][0].shape\n",
    "        action_shape = self._data_buffer[0][1].shape\n",
    "\n",
    "        obs_array = np.zeros((len(indices), *obs_shape), dtype=np.float32)\n",
    "        action_array = np.zeros((len(indices), *action_shape), dtype=np.float32)\n",
    "        reward_array = np.zeros(len(indices), dtype=np.float32)\n",
    "        next_obs_array = np.zeros((len(indices), *obs_shape), dtype=np.float32)\n",
    "        done_array = np.zeros(len(indices), dtype=np.float32)\n",
    "\n",
    "        # collect the data\n",
    "        for i, idx in enumerate(indices):\n",
    "            # get the single transition\n",
    "            data = self._data_buffer[idx]\n",
    "            obs, act, reward, next_obs, d = data\n",
    "            # store to the arrays\n",
    "            obs_array[i] = obs\n",
    "            action_array[i]= act\n",
    "            reward_array[i] = reward\n",
    "            next_obs_array[i] = next_obs\n",
    "            done_array[i] = d\n",
    "\n",
    "        if as_tensor:\n",
    "            return (\n",
    "                torch.tensor(obs_array, dtype=torch.float32),\n",
    "                torch.tensor(action_array, dtype=torch.float32),\n",
    "                torch.tensor(reward_array, dtype=torch.float32),\n",
    "                torch.tensor(next_obs_array, dtype=torch.float32),\n",
    "                torch.tensor(done_array, dtype=torch.float32),\n",
    "            )\n",
    "        return obs_array, action_array, reward_array, next_obs_array, done_array\n",
    "\n",
    "    def sample_batch(self, batch_size, as_tensor = False):\n",
    "        \"\"\" Args:\n",
    "                batch_size (int): size of the sampled batch data.\n",
    "        \"\"\"\n",
    "        # sample indices with replaced\n",
    "        \n",
    "        # batch-size check\n",
    "        if batch_size > len(self._data_buffer):\n",
    "            raise ValueError(f\"Batch size ({batch_size}) exceeds buffer size ({len(self._data_buffer)})\")\n",
    "        \n",
    "        indices = np.random.choice(len(self._data_buffer), size = batch_size, replace = False)\n",
    "        return self._encode_sample(indices, as_tensor = as_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, replay_buffer, actor, critic, \n",
    "                 actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, tau=0.005, batch_size=64, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            action_bound (float): Maximum absolute value for actions.\n",
    "            replay_buffer (ReplayBuffer): Replay buffer for experience replay.\n",
    "            actor (nn.Module): Actor network.\n",
    "            critic (nn.Module): Critic network.\n",
    "            actor_lr (float): Learning rate for the actor.\n",
    "            critic_lr (float): Learning rate for the critic.\n",
    "            gamma (float): Discount factor.\n",
    "            tau (float): Soft update factor.\n",
    "            batch_size (int): Batch size for training.\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim \n",
    "        self.action_bound = action_bound\n",
    "\n",
    "        # Replay buffer\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Actor and Target Actor\n",
    "        self.actor = actor.to(self.device)\n",
    "        self.target_actor = type(actor)(\n",
    "            input_dim=actor.input_dim,\n",
    "            first_dim=actor.first_dim,\n",
    "            second_dim=actor.second_dim,\n",
    "            n_actions=actor.n_actions,\n",
    "            action_low=actor.action_low,\n",
    "            action_high=actor.action_high,\n",
    "            alpha=actor.alpha\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Critic and Target Critic\n",
    "        self.critic = critic.to(self.device)\n",
    "        self.target_critic = type(critic)(\n",
    "            input_dim=critic.input_dim,\n",
    "            first_dim=critic.first_dim,\n",
    "            second_dim=critic.second_dim,\n",
    "            n_actions=critic.n_actions,\n",
    "            alpha=critic.alpha\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Action Bounds\n",
    "        self.action_low = actor.action_low.to(self.device)\n",
    "        self.action_high = actor.action_high.to(self.device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        # Sync target networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        # Other Parameters\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def soft_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(self.tau * source_param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "    def select_action(self, state, noise_scale=0.1, explore=True):\n",
    "        if isinstance(state, dict):\n",
    "            state = state[\"observation\"]\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state_tensor).detach().cpu().numpy()[0]\n",
    "        if explore:\n",
    "            action += noise_scale * np.random.normal(size=self.action_dim)\n",
    "        action = np.clip(action, self.action_low.cpu().numpy(), self.action_high.cpu().numpy())\n",
    "        return action\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Perform one training step for the Actor and Critic networks.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None, None\n",
    "\n",
    "        # Sample a batch of transitions\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample_batch(\n",
    "            self.batch_size, as_tensor=True\n",
    "        )\n",
    "\n",
    "        # Move sampled tensors to the device\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Update Critic\n",
    "        with torch.no_grad():\n",
    "            target_next_actions = self.target_actor(next_states)\n",
    "            target_q_values = self.target_critic(next_states, target_next_actions)\n",
    "            target_q_values = rewards + self.gamma * (1 - dones) * target_q_values.squeeze()\n",
    "\n",
    "        current_q_values = self.critic(states, actions).squeeze()\n",
    "        critic_loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update Actor\n",
    "        predicted_actions = self.actor(states)\n",
    "        actor_loss = -self.critic(states, predicted_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft update target networks\n",
    "        self.soft_update(self.target_actor, self.actor)\n",
    "        self.soft_update(self.target_critic, self.critic)\n",
    "\n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" CHANGE BASED ON DESIRED ENVIRONMENT \"\n",
    "\n",
    "env = gym.make(\"PandaReach-v3\")\n",
    "#env = gym.make(\"PandaPush-v3\")\n",
    "#env = gym.make(\"PandaSlide-v3\")\n",
    "#env = gym.make(\"PandaPickAndPlace-v3\")\n",
    "#env = gym.make(\"PandaStack-v3\")\n",
    "#env = gym.make(\"PandaFlip-v3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, episodes=100, max_steps=200, eval_interval=10):\n",
    "    rewards_log = []\n",
    "    eval_rewards_log = []\n",
    "    timestep_rewards = []\n",
    "    timestep_losses = []\n",
    "\n",
    "    timestep = 0\n",
    "    replay_buffer = agent.replay_buffer\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        raw_state, _ = env.reset()\n",
    "        state = raw_state.get(\"observation\", raw_state)\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # DEBUG\n",
    "            #print(f\"State before action selection: {state}\")  \n",
    "            action = agent.select_action(state)\n",
    "            result = env.step(action)\n",
    "            # Debug\n",
    "            #print(f\"Step result: {result}\")\n",
    "\n",
    "            raw_next_state, reward, done, _, info = env.step(action)\n",
    "            # extract next state from \"observation\"\n",
    "            next_state = raw_next_state[\"observation\"]\n",
    "\n",
    "            #next_state = raw_next_state.get(\"observation\", raw_next_state)\n",
    "            if not isinstance(next_state, np.ndarray):\n",
    "                next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            # Add the transition to the replay buffer\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            # DEBUG\n",
    "            #print(f\"Replay buffer size: {len(replay_buffer)}\")  # Debugging statement\n",
    "\n",
    "            if len(replay_buffer) < agent.batch_size:\n",
    "                continue\n",
    "            \n",
    "            # Update the networks and track losses\n",
    "            update_result = agent.update()\n",
    "            # DEBUG\n",
    "            #print(f\"Update result: {update_result}\")  \n",
    "            if update_result is None:\n",
    "                continue  # Skip this step if update() returns None\n",
    "\n",
    "            # Update the networks and track losses\n",
    "            critic_loss, actor_loss = agent.update()\n",
    "        \n",
    "            if critic_loss is not None and actor_loss is not None:\n",
    "                timestep_losses.append((timestep, (critic_loss, actor_loss)))\n",
    "                \n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            timestep += 1\n",
    "            timestep_rewards.append((timestep, episode_reward))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards_log.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_reward}\")\n",
    "\n",
    "        # Evaluate periodically\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            eval_reward = 0\n",
    "            num_episodes = 5\n",
    "            # Evaluate for number of episodes\n",
    "            for _ in range(num_episodes):  \n",
    "                raw_eval_state, _ = env.reset()\n",
    "                eval_state = raw_eval_state.get(\"observation\", raw_eval_state)\n",
    "                if not isinstance(eval_state, np.ndarray):\n",
    "                    eval_state = np.array(eval_state, dtype=np.float32)\n",
    "                eval_episode_reward = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action = agent.select_action(eval_state, explore=False)\n",
    "                    raw_eval_next_state, reward, done, _, info = env.step(action)\n",
    "                    #raw_eval_next_state, reward, done = env.step(action)\n",
    "                    #eval_next_state = raw_eval_next_state.get(\"observation\", raw_eval_next_state)\n",
    "                    eval_next_state = raw_next_state[\"observation\"]\n",
    "                    if not isinstance(eval_next_state, np.ndarray):\n",
    "                        eval_next_state = np.array(eval_next_state, dtype=np.float32)\n",
    "\n",
    "                    eval_state = eval_next_state\n",
    "                    eval_episode_reward += reward\n",
    "\n",
    "                eval_reward += eval_episode_reward\n",
    "\n",
    "            avg_eval_reward = eval_reward / num_episodes\n",
    "            eval_rewards_log.append((timestep, avg_eval_reward))\n",
    "            # DEBUG\n",
    "            print(f\"Evaluation after {episode + 1} episodes: Avg Reward = {avg_eval_reward}\")\n",
    "\n",
    "    return rewards_log, eval_rewards_log, timestep_rewards, timestep_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abduf\\AppData\\Local\\Temp\\ipykernel_10376\\4099265177.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.action_low = torch.tensor(action_low, dtype=torch.float32).to(device)\n",
      "C:\\Users\\abduf\\AppData\\Local\\Temp\\ipykernel_10376\\4099265177.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.action_high = torch.tensor(action_high, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10, Reward: -37.0\n",
      "Episode 2/10, Reward: 0.0\n",
      "Episode 3/10, Reward: -100.0\n",
      "Episode 4/10, Reward: -100.0\n",
      "Episode 5/10, Reward: -100.0\n",
      "Episode 6/10, Reward: -100.0\n",
      "Episode 7/10, Reward: -100.0\n",
      "Episode 8/10, Reward: -100.0\n",
      "Episode 9/10, Reward: -100.0\n",
      "Episode 10/10, Reward: -100.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # Environment details\n",
    "    env = gym.make(\"PandaReach-v3\")\n",
    "\n",
    "    if isinstance(env.observation_space, gym.spaces.Dict):\n",
    "        state_dim = env.observation_space[\"observation\"].shape[0]\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high[0]\n",
    "\n",
    "    # DEBUG\n",
    "    #print(f\"State Dimension: {state_dim}, Action Dimension: {action_dim}, Action Bound: {action_bound}\")\n",
    "\n",
    "    # Replay Buffer set-up\n",
    "    buffer_size = 1_000_000\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Device Configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define Actor and Critic Networks\n",
    "    hidden_dim = 256  # Number of neurons in hidden layers\n",
    "    actor = Actor(input_dim=state_dim, first_dim=hidden_dim, second_dim=hidden_dim, \n",
    "                n_actions=action_dim, action_low=env.action_space.low, \n",
    "                action_high=env.action_space.high, alpha=1e-4, device=device)\n",
    "    critic = Critic(input_dim=state_dim, first_dim=hidden_dim, second_dim=hidden_dim, \n",
    "                    n_actions=action_dim, alpha=1e-3,device=device)\n",
    "\n",
    "    # Initialize the DDPG Agent\n",
    "    agent = DDPGAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        action_bound=action_bound,\n",
    "        replay_buffer=replay_buffer,\n",
    "        actor=actor,  # Pass the Actor network\n",
    "        critic=critic,  # Pass the Critic network\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        batch_size=64\n",
    "    )\n",
    "\n",
    "    # Train and run experiments\n",
    "    train_rewards, eval_rewards, timestep_rewards, timestep_losses = train_agent(env, agent, episodes=10, max_steps=100)\n",
    "\n",
    "    # Save rewards and losses for analysis\n",
    "    np.save(\"train_rewards.npy\", train_rewards)\n",
    "    np.save(\"eval_rewards.npy\", eval_rewards)\n",
    "    np.save(\"timestep_rewards.npy\", timestep_rewards)\n",
    "    np.save(\"timestep_losses.npy\", timestep_losses)\n",
    "\n",
    "    # Visualization\n",
    "    # Cumulative rewards vs timesteps\n",
    "    timesteps, rewards = zip(*timestep_rewards)\n",
    "    plt.figure()\n",
    "    plt.plot(timesteps, rewards, label=\"Cumulative Rewards\")\n",
    "    plt.xlabel(\"Timesteps\")\n",
    "    plt.ylabel(\"Cumulative Rewards\")\n",
    "    plt.title(\"Cumulative Rewards vs Timesteps\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss vs timesteps\n",
    "    if timestep_losses:\n",
    "        timesteps, losses = zip(*timestep_losses)\n",
    "        plt.figure()\n",
    "        plt.plot(timesteps, losses, label=\"Loss\")\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss vs Timesteps\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
